{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавим и оценим какую-нибудь \"новую\" LLM. На примере Qwen/Qwen2-7B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузим токенайзер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2-7B-Instruct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на чат темплейт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим интересные нам спец токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<|im_end|>', '<|endoftext|>', '<|im_start|>'], None, '<|im_end|>', 151645)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens, tokenizer.bos_token_id, tokenizer.eos_token, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на примеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Ты классный чат-бот.<|im_end|>\n",
      "<|im_start|>user\n",
      "Сколько будет 2+2?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "4!<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt1 = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {'role': 'system', 'content': 'Ты классный чат-бот.'},\n",
    "        {'role': 'user', 'content': 'Сколько будет 2+2?'},\n",
    "        {'role': 'assistant', 'content': '4!'}\n",
    "    ], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(prompt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Сколько будет 2+2?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "4!<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt2 = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {'role': 'user', 'content': 'Сколько будет 2+2?'},\n",
    "        {'role': 'assistant', 'content': '4!'}\n",
    "    ], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Сколько будет 2+2?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt3 = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {'role': 'user', 'content': 'Сколько будет 2+2?'}\n",
    "    ], tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "print(prompt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теперь составим наш json конфиг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_config = {\n",
    "    \"system_prompt\": \"You are a helpful assistant.\",\n",
    "    \"system_message_template\": \"<|im_start|>{role}\\n{content}<|im_end|>\\n\",\n",
    "    \"user_message_template\": \"<|im_start|>{role}\\n{content}<|im_end|>\\n\",\n",
    "    \"bot_message_template\": \"<|im_start|>{role}\\n{content}<|im_end|>\\n\",\n",
    "    \"bot_message_template_incomplete\": \"<|im_start|>{role}\\n{content}\",\n",
    "    \"user_role\": \"user\",\n",
    "    \"bot_role\": \"assistant\",\n",
    "    \"system_role\": \"system\",\n",
    "    \"global_prefix\": \"\", # как мы видем он именно пустой.\n",
    "    \"suffix\": \"<|im_start|>assistant\\n\", # добавляется по аналогии с add_generation_prompt=True, если последнее сообщение не bot\n",
    "    \"add_special_tokens\": False, # почти всегда False. \n",
    "    \"eos_token\": \"<|im_end|>\" # основной критерий остановки генерации\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "\n",
    "conv_config_path = 'examples/qwen2_instruct.json'\n",
    "with codecs.open(conv_config_path, 'w', 'utf-8') as file:\n",
    "    json.dump(conv_config, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Ты классный чат-бот.<|im_end|>\n",
      "<|im_start|>user\n",
      "Сколько будет 2+2?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "4!<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llmtf.conversation import Conversation\n",
    "\n",
    "conversation = Conversation.from_template(conv_config_path)\n",
    "conversation.add_system_message('Ты классный чат-бот.')\n",
    "conversation.add_user_message('Сколько будет 2+2?')\n",
    "conversation.add_bot_message('4!')\n",
    "prompt1_our = conversation.get_prompt(add_suffix=False)\n",
    "\n",
    "assert prompt1_our == prompt1\n",
    "print(prompt1_our)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Сколько будет 2+2?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "4!<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversation = Conversation.from_template(conv_config_path)\n",
    "conversation.add_user_message('Сколько будет 2+2?')\n",
    "conversation.add_bot_message('4!')\n",
    "prompt2_our = conversation.get_prompt(add_suffix=False)\n",
    "\n",
    "assert prompt2_our == prompt2\n",
    "print(prompt2_our)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Сколько будет 2+2?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversation = Conversation.from_template(conv_config_path)\n",
    "conversation.add_user_message('Сколько будет 2+2?')\n",
    "prompt3_our = conversation.get_prompt(add_suffix=True)\n",
    "\n",
    "assert prompt3_our == prompt3\n",
    "print(prompt3_our)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отлично, conversation config готов и корректен. (Всегда проверяйте!). Теперь посчитаем какие-нибудь датасеты (часть сэмплов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Инициализируем Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmtf.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загружаем модель с полученным конфигом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-11 11:43:00 ray_utils.py:46] Failed to import Ray with ModuleNotFoundError(\"No module named 'ray'\"). For multi-node inference, please install Ray with `pip install ray`.\n",
      "INFO 06-11 11:43:00 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='Qwen/Qwen2-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=Qwen/Qwen2-7B-Instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-11 11:43:01 selector.py:51] Using XFormers backend.\n",
      "INFO 06-11 11:43:03 selector.py:51] Using XFormers backend.\n",
      "INFO 06-11 11:43:03 weight_utils.py:207] Using model weights format ['*.safetensors']\n",
      "INFO 06-11 11:43:06 model_runner.py:146] Loading model weights took 14.2487 GB\n",
      "INFO 06-11 11:43:08 gpu_executor.py:83] # GPU blocks: 60948, # CPU blocks: 4681\n",
      "INFO 06-11 11:43:11 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-11 11:43:11 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-11 11:43:18 model_runner.py:924] Graph capturing finished in 7 secs.\n",
      "INFO 06-11 11:43:18 block_manager_v1.py:247] Automatic prefix caching is enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2024-06-11 11:43:18,699: llmtf.base.vllmmodel: Set eos_token_id in generation_config to [151645]\n",
      "WARNING: 2024-06-11 11:43:18,700: llmtf.base.vllmmodel: Global prefix is equal to empty string!\n",
      "INFO: 2024-06-11 11:43:18,701: llmtf.base.vllmmodel: Model id: Qwen/Qwen2-7B-Instruct\n",
      "INFO: 2024-06-11 11:43:18,721: llmtf.base.vllmmodel: global_prefix = \n",
      "INFO: 2024-06-11 11:43:18,722: llmtf.base.vllmmodel: vllm_adds_bos = False\n",
      "INFO: 2024-06-11 11:43:18,722: llmtf.base.vllmmodel: Leading space: False\n"
     ]
    }
   ],
   "source": [
    "from llmtf.model import VLLMModel\n",
    "\n",
    "model_name_or_path = 'Qwen/Qwen2-7B-Instruct'\n",
    "model = VLLMModel(conv_config_path, device_map='cuda:0', disable_sliding_window=True, enable_prefix_caching=True)\n",
    "model.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2024-06-11 11:43:18,732: llmtf.base.evaluator: Starting eval on ['russiannlp/rucola_custom']\n",
      "INFO: 2024-06-11 11:43:18,732: llmtf.base.vllmmodel: Updated generation_config.eos_token_id: [151645]\n",
      "INFO: 2024-06-11 11:43:18,733: llmtf.base.vllmmodel: Updated generation_config.stop_strings: []\n",
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:1461: FutureWarning: The repository for RussianNLP/rucola contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/RussianNLP/rucola\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5.10k/5.10k [00:00<00:00, 14.1MB/s]\n",
      "Downloading readme: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 16.3k/16.3k [00:00<00:00, 27.4MB/s]\n",
      "Downloading data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 979k/979k [00:00<00:00, 3.98MB/s]\n",
      "Downloading data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 478k/478k [00:00<00:00, 1.40MB/s]\n",
      "Downloading data: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 370k/370k [00:00<00:00, 1.08MB/s]\n",
      "Generating train split: 7869 examples [00:00, 9793.61 examples/s] \n",
      "Generating validation split: 2787 examples [00:00, 10053.72 examples/s]\n",
      "Generating test split: 2789 examples [00:00, 10907.44 examples/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:01<00:00, 177.69it/s]\n",
      "INFO: 2024-06-11 11:43:28,960: llmtf.base.russiannlp/rucola_custom: Loading Dataset: 10.23s\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  4.00it/s]\n",
      "INFO: 2024-06-11 11:43:35,220: llmtf.base.russiannlp/rucola_custom: Processing Dataset: 6.26s\n",
      "INFO: 2024-06-11 11:43:35,221: llmtf.base.russiannlp/rucola_custom: Results for russiannlp/rucola_custom:\n",
      "INFO: 2024-06-11 11:43:35,225: llmtf.base.russiannlp/rucola_custom: {'acc': 0.725, 'mcc': 0.24077390894881104}\n",
      "INFO: 2024-06-11 11:43:35,228: llmtf.base.evaluator: Ended eval\n",
      "INFO: 2024-06-11 11:43:35,228: llmtf.base.evaluator: \n",
      "mean\trussiannlp/rucola_custom\n",
      "0.483\t0.483\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'examples/example_qwen2_7b_instruct_rucola_custom_eval'\n",
    "datasets_names = ['russiannlp/rucola_custom']\n",
    "evaluator.evaluate(model, output_dir, datasets_names=datasets_names, max_len=4000, few_shot_count=5, batch_size=8, max_sample_per_dataset=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
