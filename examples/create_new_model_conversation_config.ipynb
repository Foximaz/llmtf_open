{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавим и оценим какую-нибудь \"новую\" LLM. На примере Qwen/Qwen2-7B-Instruct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузим токенайзер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('Qwen/Qwen2-7B-Instruct')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на чат темплейт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{% for message in messages %}{% if loop.first and messages[0]['role'] != 'system' %}{{ '<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "' }}{% endif %}{{'<|im_start|>' + message['role'] + '\n",
      "' + message['content'] + '<|im_end|>' + '\n",
      "'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n",
      "' }}{% endif %}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.chat_template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим интересные нам спец токены"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['<|im_end|>', '<|endoftext|>', '<|im_start|>'], None, '<|im_end|>', 151645)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.all_special_tokens, tokenizer.bos_token_id, tokenizer.eos_token, tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Посмотрим на примеры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "Ты классный чат-бот.<|im_end|>\n",
      "<|im_start|>user\n",
      "Сколько будет 2+2?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "4!<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt1 = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {'role': 'system', 'content': 'Ты классный чат-бот.'},\n",
    "        {'role': 'user', 'content': 'Сколько будет 2+2?'},\n",
    "        {'role': 'assistant', 'content': '4!'}\n",
    "    ], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(prompt1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Сколько будет 2+2?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "4!<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt2 = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {'role': 'user', 'content': 'Сколько будет 2+2?'},\n",
    "        {'role': 'assistant', 'content': '4!'}\n",
    "    ], tokenize=False, add_generation_prompt=False\n",
    ")\n",
    "print(prompt2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Сколько будет 2+2?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt3 = tokenizer.apply_chat_template(\n",
    "    [\n",
    "        {'role': 'user', 'content': 'Сколько будет 2+2?'}\n",
    "    ], tokenize=False, add_generation_prompt=True\n",
    ")\n",
    "print(prompt3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теперь составим наш json конфиг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_config = {\n",
    "    \"system_prompt\": \"You are a helpful assistant.\",\n",
    "    \"system_message_template\": \"<|im_start|>{role}\\n{content}<|im_end|>\\n\",\n",
    "    \"user_message_template\": \"<|im_start|>{role}\\n{content}<|im_end|>\\n\",\n",
    "    \"bot_message_template\": \"<|im_start|>{role}\\n{content}<|im_end|>\\n\",\n",
    "    \"bot_message_template_incomplete\": \"<|im_start|>{role}\\n{content}\",\n",
    "    \"user_role\": \"user\",\n",
    "    \"bot_role\": \"assistant\",\n",
    "    \"system_role\": \"system\",\n",
    "    \"global_prefix\": \"\", # как мы видем он именно пустой.\n",
    "    \"suffix\": \"<|im_start|>assistant\\n\", # добавляется по аналогии с add_generation_prompt=True, если последнее сообщение не bot\n",
    "    \"add_special_tokens\": False, # почти всегда False. \n",
    "    \"eos_token\": \"<|im_end|>\" # основной критерий остановки генерации\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "\n",
    "conv_config_path = 'examples/qwen2_instruct.json'\n",
    "with codecs.open(conv_config_path, 'w', 'utf-8') as file:\n",
    "    json.dump(conv_config, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-10 17:33:28 ray_utils.py:46] Failed to import Ray with ModuleNotFoundError(\"No module named 'ray'\"). For multi-node inference, please install Ray with `pip install ray`.\n",
      "<|im_start|>system\n",
      "Ты классный чат-бот.<|im_end|>\n",
      "<|im_start|>user\n",
      "Сколько будет 2+2?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "4!<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llmtf.conversation import Conversation\n",
    "\n",
    "conversation = Conversation.from_template(conv_config_path)\n",
    "conversation.add_system_message('Ты классный чат-бот.')\n",
    "conversation.add_user_message('Сколько будет 2+2?')\n",
    "conversation.add_bot_message('4!')\n",
    "prompt1_our = conversation.get_prompt(add_suffix=False)\n",
    "\n",
    "assert prompt1_our == prompt1\n",
    "print(prompt1_our)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Сколько будет 2+2?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "4!<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversation = Conversation.from_template(conv_config_path)\n",
    "conversation.add_user_message('Сколько будет 2+2?')\n",
    "conversation.add_bot_message('4!')\n",
    "prompt2_our = conversation.get_prompt(add_suffix=False)\n",
    "\n",
    "assert prompt2_our == prompt2\n",
    "print(prompt2_our)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Сколько будет 2+2?<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conversation = Conversation.from_template(conv_config_path)\n",
    "conversation.add_user_message('Сколько будет 2+2?')\n",
    "prompt3_our = conversation.get_prompt(add_suffix=True)\n",
    "\n",
    "assert prompt3_our == prompt3\n",
    "print(prompt3_our)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отлично, conversation config готов и корректен. (Всегда проверяйте!). Теперь посчитаем какие-нибудь датасеты (часть сэмплов)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Инициализируем Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmtf.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Загружаем модель с полученным конфигом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-10 17:33:28 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='Qwen/Qwen2-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=Qwen/Qwen2-7B-Instruct)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-10 17:33:29 selector.py:51] Using XFormers backend.\n",
      "INFO 06-10 17:33:31 selector.py:51] Using XFormers backend.\n",
      "INFO 06-10 17:33:31 weight_utils.py:207] Using model weights format ['*.safetensors']\n",
      "INFO 06-10 17:33:34 model_runner.py:146] Loading model weights took 14.2487 GB\n",
      "INFO 06-10 17:33:36 gpu_executor.py:83] # GPU blocks: 60948, # CPU blocks: 4681\n",
      "INFO 06-10 17:33:39 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-10 17:33:39 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-10 17:33:48 model_runner.py:924] Graph capturing finished in 8 secs.\n",
      "INFO 06-10 17:33:48 block_manager_v1.py:247] Automatic prefix caching is enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2024-06-10 17:33:48,900: llmtf.base.llm: Override eos_token_id in generation_config from 151645 to 151645\n",
      "WARNING: 2024-06-10 17:33:48,901: llmtf.base.llm: Global prefix is equal to empty string!\n",
      "INFO: 2024-06-10 17:33:48,901: llmtf.base.llm: Model id: Qwen/Qwen2-7B-Instruct\n",
      "INFO: 2024-06-10 17:33:48,923: llmtf.base.llm: global_prefix = \n",
      "INFO: 2024-06-10 17:33:48,924: llmtf.base.llm: vllm_adds_bos = False\n",
      "INFO: 2024-06-10 17:33:48,925: llmtf.base.llm: Resetting generation_config.stop_strings to []\n",
      "INFO: 2024-06-10 17:33:48,925: llmtf.base.llm: Leading space: False\n"
     ]
    }
   ],
   "source": [
    "from llmtf.model import VLLMModel\n",
    "\n",
    "model_name_or_path = 'Qwen/Qwen2-7B-Instruct'\n",
    "model = VLLMModel(conv_config_path, device_map='cuda:0', disable_sliding_window=True, enable_prefix_caching=True)\n",
    "model.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:01<00:00, 175.95it/s]\n",
      "INFO: 2024-06-10 17:33:52,961: llmtf.base.russiannlp/rucola_custom: Loading Dataset: 4.03s\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.98it/s]\n",
      "INFO: 2024-06-10 17:33:59,243: llmtf.base.russiannlp/rucola_custom: Processing Dataset: 6.28s\n",
      "INFO: 2024-06-10 17:33:59,244: llmtf.base.russiannlp/rucola_custom: Results for russiannlp/rucola_custom:\n",
      "INFO: 2024-06-10 17:33:59,248: llmtf.base.llm: Resetting generation_config.stop_strings to []\n",
      "INFO: 2024-06-10 17:33:59,249: llmtf.base.russiannlp/rucola_custom: {'acc': 0.725, 'mcc': 0.24077390894881104}\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'examples/example_qwen2_7b_instruct_rucola_custom_eval'\n",
    "datasets_names = ['russiannlp/rucola_custom']\n",
    "evaluator.evaluate(model, output_dir, datasets_names=datasets_names, max_len=4000, few_shot_count=5, batch_size=8, max_sample_per_dataset=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
