{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Пример на основе RoCoLa\n",
    "### В качестве теста выступает валидация, в качестве примеров для few-shot выступает трейн."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llmtf.base import Task, SimpleFewShotHFTask, LLM\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "from datasets import load_dataset, Dataset\n",
    "import copy\n",
    "from llmtf.metrics import mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Для создания новой таски необходимо, чтобы она была наследником класса Task с реализованными функциями\n",
    "\n",
    "```python\n",
    "def __init__(self, **kwargs):\n",
    "    # Важно определить как минимум 2 переменные: self.method и self._max_new_token.\n",
    "    # self.method - Это режим работы. Либо calculate_tokens_proba либо generate. Метод, который будет вызываться у LLM. \n",
    "    # self._max_new_token - дефолтное значение 64, если нужно генерить больше или меньше, стоит поменять на нужное значение. Для calculate_tokens_proba достаточно 1 токена.\n",
    "\n",
    "    self.method = 'calculate_tokens_proba'\n",
    "    self._max_new_tokens = 1\n",
    "\n",
    "@property\n",
    "def name(self) -> str:\n",
    "    # Возвращает имя датасета, которое будет записываться потом в логи.\n",
    "\n",
    "@property\n",
    "def choices(self) -> List:\n",
    "    # ТОЛЬКО для случая calculate_tokens_proba. Вероятность на каких токенах оцениваем?\n",
    "\n",
    "def evaluate(self, sample, y_pred) -> Dict:\n",
    "    # Принимает на вход результат пару sample и LLM.method. По ним необхоимо посчитать критерий правильности результата работы модели и вернуть его (или несколько) как Dict\n",
    "    # Например {'acc': sample['outputs'] == y_pred}. Имя 'acc' будет потом использоваться в методе aggregation.\n",
    "\n",
    "def aggregation(self) -> Dict:\n",
    "    # Возвращает Dict из имя -> Callable. В дальнейшем этот Callable вызывается для списка результатов, в котором каждый обработан методом evaluate.\n",
    "    # Например {\"acc\": np.mean} означает, что в метод np.mean будет подан список из результата evaluate по ключу \"acc\".\n",
    "\n",
    "def leaderboard_aggregation(self, metrics: Dict) -> float:\n",
    "    # итоговая метрика по датасету. Это может быть mean, или определенная выбранная метрика. На вход идет Dict со значениями метрик, например, {\"acc\": 0.5, \"f1\": 0.5}, необходимо вернуть число.\n",
    "\n",
    "def load_dataset(self, **kwargs) -> Tuple[List[Dict], List[Dict]]:\n",
    "    # Основная логика подготовки данных тут. load_dataset возвращает пару списков: messages и samples. То есть задача объекта класса Task в том, чтобы преобразовать вашу задачу в формат сообщений.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Для упрощения добавления новой таски реализован класс SimpleFewShotHFTask, который последовательно проходит по huggingface dataset и формирует список messages. При наследовании от него необходимо реализовать несколько простых функций:\n",
    "\n",
    "```python\n",
    "def dataset_args(self) -> Dict:\n",
    "    # передается в метод datasets.load_dataset(**self.dataset_args())\n",
    "\n",
    "def test_split_name(self) -> str:\n",
    "    # какой сплит мы считаем за тестовый?\n",
    "\n",
    "def prompt_split_name(self) -> str:\n",
    "    # из какого сплита мы формируем few-shot примеры?\n",
    "\n",
    "def create_messages(self, sample, with_answer) -> List[Dict]:\n",
    "    # по сути главный метод. Преобразует пример из датасета (строчку) в набор сообщений. with_answer - для случая, когда мы можем добавть ответ в сообщения, например, когда формируем few-shot примеры.\n",
    "```\n",
    "#### и опционально:\n",
    "```python\n",
    "def prompt_dataset_start_idx(self) -> int:\n",
    "    # первый индекс промпт сплита для формирования few-shot примеров. Они используются один за одним последовательно начиная с возвращаемого тут параметра. В дефолтной реализации = 0.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Теперь посмотрим на RoCoLa\n",
    "Каждый sample датасета в [RuCoLa](https://huggingface.co/datasets/RussianNLP/rucola) состоит из нескольких полей, из которых нам интересны sentence и label. На их основе и будет формироваться messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RuColaCustomTask(SimpleFewShotHFTask):\n",
    "    RUCOLA_HF_PATH = 'RussianNLP/rucola'\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.method = 'calculate_tokens_proba'\n",
    "        self._max_new_tokens = 1\n",
    "\n",
    "    def evaluate(self, sample, y_pred) -> Dict:\n",
    "        y_true = str(sample['label'])\n",
    "        y_pred = sorted([pair for pair in y_pred.items()], key=lambda x: -x[1])[0][0]\n",
    "        return {\"acc\": y_true == y_pred, \"mcc\": [y_true, y_pred]}\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return 'russiannlp/rucola_custom'\n",
    "    \n",
    "    @property\n",
    "    def choices(self):\n",
    "        return [\"0\", \"1\"]\n",
    "\n",
    "    def aggregation(self) -> Dict:\n",
    "        return {\"acc\": mean, \"mcc\": lambda data: matthews_corrcoef([d[0] for d in data], [d[1] for d in data])}\n",
    "\n",
    "    def dataset_args(self) -> Dict:\n",
    "        return {'path': self.RUCOLA_HF_PATH}\n",
    "\n",
    "    def test_split_name(self) -> str:\n",
    "        return 'validation'\n",
    "\n",
    "    def prompt_split_name(self) -> str:\n",
    "        return 'train'\n",
    "\n",
    "    def create_messages(self, sample, with_answer):\n",
    "        messages = []\n",
    "        instruction_user = 'Твоя задача определить приемлемость текста для русского языка с точки зрения синтаксиса, морфологии и семантики. Ответом должно служить одно число: 0 или 1, где 0 - предложение не приемлемо с точки зрения русского языка, 1 - приемлемо.\\nТекст: {sentence}'\n",
    "        instruction_bot = 'Ответ: {label}'\n",
    "        instruction_bot_incomplete = 'Ответ:'\n",
    "\n",
    "        bot_content = instruction_bot.format(**sample) if with_answer else instruction_bot_incomplete\n",
    "\n",
    "        messages.append({'role': 'user', 'content': instruction_user.format(**sample)})\n",
    "        messages.append({'role': 'bot', 'content': bot_content})\n",
    "\n",
    "        return messages\n",
    "\n",
    "    def prompt_dataset_start_idx(self) -> int:\n",
    "        # в ближайших индексах после 29 сбалансировано по меткам классов, вот поэтому\n",
    "        return 29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Этот код можно (как уже и сделано) добавить в поддиректорию tasks и затем записать в объект TASK_REGISTRY в ```tasks.__init__```. Но помимо этого можно воспользоваться методом add_new_task в Evaluator во время исполнения, как будет показано далее."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmtf.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator()\n",
    "evaluator.add_new_task('OurRuColaCustomTask', RuColaCustomTask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Инициализация LLM. В данном случае с использованием VLLM фреймворка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 06-10 17:34:23 ray_utils.py:46] Failed to import Ray with ModuleNotFoundError(\"No module named 'ray'\"). For multi-node inference, please install Ray with `pip install ray`.\n",
      "INFO 06-10 17:34:23 llm_engine.py:161] Initializing an LLM engine (v0.4.3) with config: model='openchat/openchat-3.5-0106', speculative_config=None, tokenizer='openchat/openchat-3.5-0106', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, rope_scaling=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), seed=0, served_model_name=openchat/openchat-3.5-0106)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/transformers/generation/configuration_utils.py:515: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 06-10 17:34:24 selector.py:51] Using XFormers backend.\n",
      "INFO 06-10 17:34:26 selector.py:51] Using XFormers backend.\n",
      "INFO 06-10 17:34:26 weight_utils.py:207] Using model weights format ['*.safetensors']\n",
      "INFO 06-10 17:34:29 model_runner.py:146] Loading model weights took 13.4917 GB\n",
      "INFO 06-10 17:34:29 gpu_executor.py:83] # GPU blocks: 29181, # CPU blocks: 2048\n",
      "INFO 06-10 17:34:31 model_runner.py:854] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 06-10 17:34:31 model_runner.py:858] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "INFO 06-10 17:34:39 model_runner.py:924] Graph capturing finished in 8 secs.\n",
      "INFO 06-10 17:34:39 block_manager_v1.py:247] Automatic prefix caching is enabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: 2024-06-10 17:34:40,334: llmtf.base.llm: Override eos_token_id in generation_config from 32000 to 32000\n",
      "INFO: 2024-06-10 17:34:40,335: llmtf.base.llm: Model id: openchat/openchat-3.5-0106\n",
      "INFO: 2024-06-10 17:34:40,336: llmtf.base.llm: _check_if_leading_space: \"[28705, 28740]\"\n",
      "INFO: 2024-06-10 17:34:40,356: llmtf.base.llm: global_prefix = <s>\n",
      "INFO: 2024-06-10 17:34:40,356: llmtf.base.llm: vllm_adds_bos = True\n",
      "INFO: 2024-06-10 17:34:40,357: llmtf.base.llm: Resetting generation_config.stop_strings to []\n",
      "INFO: 2024-06-10 17:34:40,357: llmtf.base.llm: Leading space: True\n"
     ]
    }
   ],
   "source": [
    "from llmtf.model import VLLMModel\n",
    "\n",
    "conv_path = 'conversation_configs/openchat_3.5_1210.json'\n",
    "model_name_or_path = 'openchat/openchat-3.5-0106'\n",
    "model = VLLMModel(conv_path, device_map='cuda:0', disable_sliding_window=True, enable_prefix_caching=True)\n",
    "model.from_pretrained(model_name_or_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 294.67it/s]\n",
      "INFO: 2024-06-10 17:34:44,207: llmtf.base.russiannlp/rucola_custom: Loading Dataset: 3.84s\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 25/25 [00:06<00:00,  3.92it/s]\n",
      "INFO: 2024-06-10 17:34:50,594: llmtf.base.russiannlp/rucola_custom: Processing Dataset: 6.39s\n",
      "INFO: 2024-06-10 17:34:50,594: llmtf.base.russiannlp/rucola_custom: Results for russiannlp/rucola_custom:\n",
      "INFO: 2024-06-10 17:34:50,598: llmtf.base.llm: Resetting generation_config.stop_strings to []\n",
      "INFO: 2024-06-10 17:34:50,598: llmtf.base.russiannlp/rucola_custom: {'acc': 0.735, 'mcc': 0.21467076918233885}\n"
     ]
    }
   ],
   "source": [
    "output_dir = 'examples/example_rucula_custom_openchat_3.5_0106_eval'\n",
    "datasets_names = ['OurRuColaCustomTask']\n",
    "evaluator.evaluate(model, output_dir, datasets_names=datasets_names, max_len=4000, few_shot_count=5, batch_size=8, max_sample_per_dataset=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
