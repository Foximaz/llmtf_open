{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmtf.model import HuggingFaceLLM\n",
    "from llmtf.task import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95b1a7d4ecd94026bffa979ee468a047",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mtikhomi/.virtualenvs/llm_dev/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/home/mtikhomi/.virtualenvs/llm_dev/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:410: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.5` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "INFO:llmtf.model:Model id: openchat/openchat-3.5-1210, params: 7241748480, dtype: torch.bfloat16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|end_of_turn|>\n",
      "32000\n",
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"max_length\": 2048,\n",
      "  \"max_new_tokens\": 256,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"temperature\": 0.1,\n",
      "  \"top_k\": 40,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = HuggingFaceLLM(conversation_template_path='conversation_configs/openchat_3.5_1210.json')\n",
    "model.from_pretrained('openchat/openchat-3.5-1210')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HuggingFaceLLM(conversation_template_path='conversation_configs/llama3.json')\n",
    "model.from_pretrained('NousResearch/Meta-Llama-3-8B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:00<00:00, 341.17it/s]\n",
      "100%|██████████| 220/220 [00:34<00:00,  6.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darumeru/RCB\n",
      "{'acc': 0.5409090909090909, 'f1_macro': 0.4905195370865565}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llmtf.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator()\n",
    "evaluator.evaluate(model, output_dir='test_evaluation', datasets_names=['RCB'], few_shot_count=5, max_len=2048, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1046/1046 [00:00<00:00, 1811.29it/s]\n",
      "100%|██████████| 1046/1046 [11:10<00:00,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darumeru/MultiQ\n",
      "{'f1': 0.222524223492856, 'em': 0.11663479923518165}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llmtf.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator()\n",
    "evaluator.evaluate(model, output_dir='test_evaluation', datasets_names=['RCB'], few_shot_count=0, max_len=2048, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1046/1046 [00:00<00:00, 2641.11it/s]\n",
      "100%|██████████| 262/262 [08:25<00:00,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darumeru/MultiQ\n",
      "{'f1': 0.22144599830902306, 'em': 0.1147227533460803}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llmtf.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator()\n",
    "evaluator.evaluate(model, output_dir='test_evaluation', datasets_names=['MultiQ'], few_shot_count=0, max_len=2048, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1046/1046 [00:03<00:00, 310.22it/s]\n",
      "100%|██████████| 262/262 [06:51<00:00,  1.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darumeru/MultiQ\n",
      "{'f1': 0.4423165727546076, 'em': 0.36424474187380496}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llmtf.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator()\n",
    "evaluator.evaluate(model, output_dir='test_evaluation', datasets_names=['MultiQ'], few_shot_count=5, max_len=2048, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1046/1046 [00:03<00:00, 305.72it/s]\n",
      "100%|██████████| 1046/1046 [07:03<00:00,  2.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darumeru/MultiQ\n",
      "{'f1': 0.440798317268857, 'em': 0.361376673040153}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llmtf.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator()\n",
    "evaluator.evaluate(model, output_dir='test_evaluation', datasets_names=['MultiQ'], few_shot_count=5, max_len=2048, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:00<00:00, 930.62it/s]\n",
      "100%|██████████| 55/55 [00:16<00:00,  3.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darumeru/RCB\n",
      "{'acc': 0.5409090909090909, 'f1_macro': 0.4603089533417402}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llmtf.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator()\n",
    "evaluator.evaluate(model, output_dir='test_evaluation', datasets_names=['MultiQ'], few_shot_count=2, max_len=8192, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:00<00:00, 1029.71it/s]\n",
      "100%|██████████| 220/220 [00:16<00:00, 13.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darumeru/RCB\n",
      "{'acc': 0.5454545454545454, 'f1_macro': 0.4693798345287699}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llmtf.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator()\n",
    "evaluator.evaluate(model, output_dir='test_evaluation', datasets_names=['RCB'], few_shot_count=2, max_len=8192, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:00<00:00, 992.93it/s] \n",
      "100%|██████████| 110/110 [00:16<00:00,  6.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darumeru/RCB\n",
      "{'acc': 0.5409090909090909, 'f1_macro': 0.4603089533417402}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llmtf.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator()\n",
    "evaluator.evaluate(model, output_dir='test_evaluation', datasets_names=['RCB'], few_shot_count=2, max_len=8192, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220/220 [00:00<00:00, 1014.84it/s]\n",
      "100%|██████████| 28/28 [00:16<00:00,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "darumeru/RCB\n",
      "{'acc': 0.5409090909090909, 'f1_macro': 0.4650088340249445}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llmtf.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator()\n",
    "evaluator.evaluate(model, output_dir='test_evaluation', datasets_names=['RCB'], few_shot_count=2, max_len=8192, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s: [1, 2, 3]\n",
      "s: [4, 5]\n"
     ]
    }
   ],
   "source": [
    "some_list = [1, 2, 3, 4, 5]\n",
    "bs = 3\n",
    "for i in range(0, len(some_list), bs):\n",
    "    print(f's: {some_list[i:i+bs]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'m': [1, 2], 'p': ['a', 'b']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [{'m': 1, 'p': 'a'}, {'m': 2, 'p': 'b'}]\n",
    "\n",
    "{k: [subdict[k] for subdict in data] for k in data[0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmtf.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator()\n",
    "evaluator.evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmtf.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator()\n",
    "evaluator.evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmtf.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator()\n",
    "evaluator.evaluate(model, datasets_names=['ruTiE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_of_interest_id = 0\n",
    "tokens_of_interest = [' A', ' B']\n",
    "tokens_of_interest = ['1', '2']\n",
    "\n",
    "tokens = [model.tokenizer(t, add_special_tokens=model.conversation_template['add_special_tokens']).input_ids for t in tokens_of_interest]\n",
    "print(tokens)\n",
    "for token in tokens:\n",
    "    print(model.tokenizer.convert_ids_to_tokens(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "searching_token = 'A'#tokens_of_interest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = 'Ответ:'\n",
    "prompt_check = prompt + ' ' + searching_token#tokens_of_interest[0]\n",
    "prompt_check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shift = len(model.tokenizer(prompt, add_special_tokens=model.conversation_template['add_special_tokens']).input_ids)\n",
    "shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_rest = model.tokenizer(prompt_check, add_special_tokens=model.conversation_template['add_special_tokens']).input_ids[shift:]\n",
    "skip = 0\n",
    "is_ok = False\n",
    "for token in tokens_rest:\n",
    "    if len(model.tokenizer.decode([token]).strip()) > 0 and searching_token.startswith(model.tokenizer.decode([token]).strip()):\n",
    "        is_ok = True\n",
    "        break\n",
    "    skip += 1\n",
    "\n",
    "assert skip <= 1 and is_ok\n",
    "token, skip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tokenizer.convert_ids_to_tokens([362])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_of_interest[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_rest[:skip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tokenizer.decode([token])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(model.tokenizer.decode([token]).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) модель ставит пробел -> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1) добавлять пробел к промпту?\n",
    "2) index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmtf.evaluator import Evaluator\n",
    "\n",
    "evaluator = Evaluator()\n",
    "evaluator.evaluate(model, datasets_names=['ruTiE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator()\n",
    "evaluator.evaluate(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task = ruMMLU()\n",
    "messages, samples = task.load_dataset(model, 4096, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, y_pred = getattr(model, task.method)(**messages[0])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[0]['sample']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.evaluate(samples[0]['sample'], y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "metrics = []\n",
    "for i in tqdm(range(100)):\n",
    "    prompt, y_pred = getattr(model, task.method)(**messages[i])\n",
    "    metrics.append(task.evaluate(samples[i]['sample'], y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.aggregation()['acc']([m['acc'] for m in metrics])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt, y_pred = getattr(model, task.method)(messages=samples[0], tokens_of_interest=task.choices)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted([pair for pair in y_pred.items()], key=lambda x: -x[1])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [getattr(model, task.method)(**m) for m in sample]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
